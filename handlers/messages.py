"""
Handler de mensajes de usuario con integraci√≥n de AI.
"""

import json
from telegram import Update
from telegram.ext import ContextTypes
from google.genai import types
from config import AI_PROVIDER, logger
from ai.providers import generate_ai_response
from ai.prompts import SYSTEM_INSTRUCTION
from ai.tools import execute_function
from core.session_manager import get_or_create_session, clear_session, MAX_HISTORY_MESSAGES, user_sessions


async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    Maneja todos los mensajes del usuario usando AI (Gemini o ChatGPT) con Function Calling
    y gesti√≥n de sesiones por usuario para mantener contexto conversacional.
    """
    user_message = update.message.text
    user_id = update.effective_user.id
    
    logger.info(f"Usuario {user_id}: {user_message}")
    
    # Obtener o crear sesi√≥n de chat
    chat_session = get_or_create_session(user_id)
    
    try:
        # Generar respuesta con el AI provider configurado
        response = generate_ai_response(user_message, chat_session)
        
        # ===== PROCESAR RESPUESTA SEG√öN PROVIDER =====
        if AI_PROVIDER == "chatgpt":
            await _process_chatgpt_response(response, user_message, user_id, update)
        else:
            await _process_gemini_response(response, chat_session, update)
        
    except Exception as e:
        logger.error(f"‚ùå Error procesando mensaje (user_id={user_id}): {e}")
        import traceback
        error_traceback = traceback.format_exc()
        logger.error(error_traceback)
        
        await _handle_error(e, user_id, update)


async def _process_chatgpt_response(response, user_message: str, user_id: int, update: Update):
    """Procesa la respuesta de ChatGPT."""
    message = response.choices[0].message
    
    # Guardar mensaje del usuario en el historial
    user_sessions[user_id].append({"role": "user", "content": user_message})
    
    # Limitar el historial a los √∫ltimos MAX_HISTORY_MESSAGES mensajes
    if len(user_sessions[user_id]) > MAX_HISTORY_MESSAGES:
        # Mantener solo los √∫ltimos mensajes (ventana deslizante)
        user_sessions[user_id] = user_sessions[user_id][-MAX_HISTORY_MESSAGES:]
        logger.info(f"üì¶ Historial limitado a {MAX_HISTORY_MESSAGES} mensajes para usuario {user_id}")
    
    if message.tool_calls:
        # Hay llamadas a funciones - recopilar resultados
        function_results = []
        for tool_call in message.tool_calls:
            function_name = tool_call.function.name
            function_args = json.loads(tool_call.function.arguments)
            
            logger.info(f"ü§ñ ChatGPT llama a funci√≥n: {function_name} con args: {function_args}")
            
            # Ejecutar funci√≥n y obtener resultado
            function_result = await execute_function(function_name, function_args)
            function_results.append({
                "tool_call_id": tool_call.id,
                "role": "tool",
                "name": function_name,
                "content": function_result
            })
        
        # Guardar mensaje del asistente con tool_calls en historial
        user_sessions[user_id].append({
            "role": "assistant",
            "content": message.content if message.content else "",
            "tool_calls": [{
                "id": tc.id,
                "type": "function",
                "function": {"name": tc.function.name, "arguments": tc.function.arguments}
            } for tc in message.tool_calls]
        })
        
        # Agregar resultados de funciones al historial
        user_sessions[user_id].extend(function_results)
        
        # Hacer segunda llamada a ChatGPT para que procese los resultados y genere respuesta natural
        logger.info("üîÑ Enviando resultados a ChatGPT para generar respuesta natural")
        
        # Crear mensajes solo con historial + system instruction (sin mensaje vac√≠o del usuario)
        from config import openai
        messages_with_results = [{"role": "system", "content": SYSTEM_INSTRUCTION}]
        messages_with_results.extend(user_sessions[user_id])
        
        second_response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages_with_results
        )
        final_message = second_response.choices[0].message
        
        if final_message.content and final_message.content.strip():
            await update.message.reply_text(final_message.content, parse_mode='Markdown')
            user_sessions[user_id].append({"role": "assistant", "content": final_message.content})
        else:
            # Fallback: Si la IA no gener√≥ respuesta de texto, enviar confirmaci√≥n natural
            logger.warning("‚ö†Ô∏è ChatGPT no retorn√≥ contenido de texto despu√©s de function call")
            fallback_msg = "Listo ‚úÖ"
            await update.message.reply_text(fallback_msg)
            user_sessions[user_id].append({"role": "assistant", "content": fallback_msg})
    
    
    elif message.content:
        # Respuesta de texto normal
        await update.message.reply_text(message.content, parse_mode='Markdown')
        
        # Guardar respuesta del asistente en el historial
        user_sessions[user_id].append({"role": "assistant", "content": message.content})
    else:
        await update.message.reply_text(
            "ü§î No estoy seguro de c√≥mo ayudarte con eso. ¬øPuedes reformular tu pregunta?"
        )
        user_sessions[user_id].append({
            "role": "assistant", 
            "content": "ü§î No estoy seguro de c√≥mo ayudarte con eso. ¬øPuedes reformular tu pregunta?"
        })


async def _process_gemini_response(response, chat_session, update: Update):
    """Procesa la respuesta de Gemini."""
    if response.candidates and response.candidates[0].content.parts:
        # Buscar si hay function calls y recopilar resultados
        function_calls_found = []
        text_responses = []
        
        for part in response.candidates[0].content.parts:
            # Si hay una llamada a funci√≥n
            if hasattr(part, 'function_call') and part.function_call:
                function_call = part.function_call
                function_name = function_call.name
                function_args = dict(function_call.args)
                
                logger.info(f"ü§ñ Gemini llama a funci√≥n: {function_name} con args: {function_args}")
                
                # Ejecutar funci√≥n y guardar resultado
                function_result = await execute_function(function_name, function_args)
                function_calls_found.append({
                    "name": function_name,
                    "result": function_result
                })
            
            # Si es solo texto
            elif hasattr(part, 'text') and part.text:
                text_responses.append(part.text)
        
        # Si hubo function calls, enviar resultados de vuelta a Gemini para respuesta natural
        if function_calls_found:
            logger.info("üîÑ Enviando resultados a Gemini para generar respuesta natural")
            
            # Construir mensaje con los resultados de las funciones
            has_sent_response = False
            for fc in function_calls_found:
                try:
                    # Enviar resultado de funci√≥n de vuelta al chat
                    function_response_part = types.Part.from_function_response(
                        name=fc["name"],
                        response={"result": fc["result"]}
                    )
                    
                    # Obtener respuesta final de Gemini procesando el resultado
                    final_response = chat_session.send_message(function_response_part)
                    
                    # Enviar la respuesta natural al usuario
                    if final_response.candidates and final_response.candidates[0].content.parts:
                        for final_part in final_response.candidates[0].content.parts:
                            if hasattr(final_part, 'text') and final_part.text and final_part.text.strip():
                                await update.message.reply_text(final_part.text, parse_mode='Markdown')
                                has_sent_response = True
                except Exception as func_error:
                    logger.error(f"‚ùå Error procesando resultado de funci√≥n {fc['name']}: {func_error}")
                    # Continuar con la siguiente funci√≥n
            
            # Si no se envi√≥ ninguna respuesta, enviar confirmaci√≥n natural
            if not has_sent_response:
                logger.warning("‚ö†Ô∏è Gemini no retorn√≥ contenido de texto despu√©s de function calls")
                await update.message.reply_text("Listo ‚úÖ")
        
        # Si solo hay texto (sin function calls)
        elif text_responses:
            for text in text_responses:
                await update.message.reply_text(text, parse_mode='Markdown')
    
    # Si no hay partes en la respuesta
    else:
        await update.message.reply_text(
            "ü§î No estoy seguro de c√≥mo ayudarte con eso. ¬øPuedes reformular tu pregunta?"
        )


async def _handle_error(e: Exception, user_id: int, update: Update):
    """
    Manejo de errores inteligente.
    Solo reinicia sesi√≥n si es un error cr√≠tico de API.
    Errores menores no deber√≠an borrar el historial.
    """
    error_str = str(e).lower()
    critical_errors = [
        'invalid api key',
        'quota exceeded',
        'rate limit',
        'authentication',
        'unauthorized',
        'invalid_request_error'
    ]
    
    # Verificar si es un error cr√≠tico que requiere reinicio
    is_critical = any(err in error_str for err in critical_errors)
    
    if is_critical:
        # Error cr√≠tico: Reiniciar sesi√≥n
        logger.warning(f"üîÑ Error CR√çTICO detectado. Reiniciando sesi√≥n para usuario {user_id}")
        
        clear_session(user_id)
        
        # Crear nueva sesi√≥n
        get_or_create_session(user_id)
        
        await update.message.reply_text(
            "‚ö†Ô∏è borrando historial de conversacion"
        )
    else:
        # Error menor: Mantener sesi√≥n pero informar al usuario
        logger.warning(f"‚ö†Ô∏è Error menor detectado. Manteniendo sesi√≥n para usuario {user_id}")
        await update.message.reply_text(
            "‚ùå Hubo un problema procesando tu mensaje. Tu historial se mantiene intacto.\n"
            "Por favor, intenta de nuevo o reformula tu pregunta."
        )
